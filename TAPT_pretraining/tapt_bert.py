# -*- coding: utf-8 -*-
"""TAPT_Bert.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TSGE7q7ANRKy_2R9cGL1POAmm-J8fPih
"""


import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import data
from transformers import  BertTokenizer, BertModel , BertForMaskedLM
import jieba # wwm whole word mask(对一个词要mask全mask)
import pandas as pd
import random
import numpy as np


class BERT_PT_Dataset(data.Dataset):
    # 学习源码
    # https://github.com/google-research/bert/blob/master/create_pretraining_data.py
    # https://github.com/brightmart/roberta_zh/blob/master/create_pretraining_data.py
    # https://github.com/codertimo/BERT-pytorch/blob/d10dc4f9d5a6f2ca74380f62039526eb7277c671/bert_pytorch/dataset/dataset.py
    '''
        为预训练Bert用
    '''
    def __init__(self, corpus, tokenizer,maxi_len = 300, If_WWM = True , shrink_vocab_size = None, P_threshold = 0.15):
        assert isinstance(corpus, list) or isinstance(corpus , np.ndarray)
        self.corpus = corpus # list or np.array
        self.max_sentence_len = maxi_len
        self.tokenizer = tokenizer # Bert tokenizer
        self.If_WWM = If_WWM
        if shrink_vocab_size is not None and shrink_vocab_size < tokenizer.vocab_size:
            self.shrink_vocab_size = shrink_vocab_size
        else:
            self.shrink_vocab_size = tokenizer.vocab_size
        self.P_threshold = P_threshold


    def __getitem__(self, index):
        '''
        :param index:
        :return:
        '''
        # 原句: ”我不想再做这样没有创造性的脏活了。模型训练真没意思"
        # sentence ： [101 (开始符),我不想再做这样没有创造性的[mask][mask]了。模型训练真没,[mask][mask] ,102 终止符]
        # sentence = self.tokenizer.encode( self.dat[index] , add_special_tokens=True , max_length=self.max_sentence_len ,pad_to_max_length=True)
        sentence = self.corpus[index].replace(' ','') # 去空格(反正tokenizer也要去)
        return self.blank_setter(sentence)


    def blank_setter(self, sentence:str):
        # 完形填空出题人, 仅 MLM 非NSP
        assert isinstance(sentence,str)
        if not self.If_WWM: # NOT Whole Word Mask, 随便整
            encode_dict = self.tokenizer.encode_plus(sentence,
                                                     truncation=True, add_special_tokens=True,
                                                     max_length=self.max_sentence_len, padding='max_length')

            sentence_id = encode_dict['input_ids']  # 带开头跟结尾， 且带padding
            mask = encode_dict['attention_mask']  # [1,1,1,1...,1(true_seq_len + 2 ([cls] & [sep]),0,0,...,0]
            output_label = [-100] * len(sentence_id)  # # [-100 , [masked]/[random] 的原始字符 , -100 , -100 ] , -100 为默认省略计算loss值

            sequence = self.tokenizer.encode(sentence, truncation=True, max_length=self.max_sentence_len,
                                             padding=False)  # 真实序列

            for i in range(1 , len(sequence)-1): # 抛去[CLS] , [SEP]
                prob = random.random()
                if sentence_id[i] >= self.shrink_vocab_size:
                    sentence_id[i] = self.tokenizer.unk_token_id
                if prob < 0.15:
                    output_label[i] = sentence_id[i]  # record true character requires PREDICTION!!!
                    prob /= 0.15
                    # 80% randomly change token to mask token
                    if prob < 0.8:
                        sentence_id[i] = self.tokenizer.mask_token_id
                    # 10% randomly change token to random token
                    elif prob < 0.9:
                        sentence_id[i] = random.randrange(700, min(8000, self.shrink_vocab_size) ) #汉字范围
                    # 10% randomly change token to current token
                    else:
                        # nothing to do
                        pass
                        # tokens[i] = self.tokenizer.vacob.get(token, self.tokenizer.unk_index )
            return torch.tensor(sentence_id), torch.tensor(mask) , torch.tensor(output_label)
        else: # wwm
            # 哈工大法
            tokens_wwm = jieba.lcut(sentence)
            sentence_id = []
            output_label = []
            Prob_table = {1:0.15,2:0.278,3:0.386,4:0.478,5:0.56, 6:0.62,7:0.67,8:0.72} # 0.15
            for phrase in tokens_wwm:
                prob = random.random()
                sequence = tokenizer.encode(phrase , add_special_tokens = False) # [105,1087,xxx,100, 999] 不带[CLS] , [SEP]
                self.ForShrinkVocab(sequence) # list in-place 操作
                if prob < Prob_table.get( len(sequence), 0):
                    prob /= Prob_table[len(sequence)]
                    if prob < 0.8:
                        sentence_id.extend([self.tokenizer.mask_token_id]*len(sequence) )
                    elif prob < 0.9:
                        for _ in range(len(sequence)):
                            random_num = random.randrange(700, min(8000, self.shrink_vocab_size) )
                            sentence_id.append(random_num)
                    else:
                        sentence_id.extend(sequence)
                    output_label.extend(sequence) # 需要预测出来
                else:
                    sentence_id.extend(sequence)
                    output_label.extend([-100] * len(sequence))

                if len(sentence_id) >= self.max_sentence_len-2: # 超出maxi_len限制
                    sentence_id = sentence_id[0:self.max_sentence_len-2]
                    output_label = output_label[0:self.max_sentence_len-2]
                    break
            assert len(sentence_id) == len(output_label)
            total = len(sentence_id)
            mask = [1] * (total +2) + [0] * (self.max_sentence_len-2-total )
            sentence_id = [101] + sentence_id + [102] + [0] * (self.max_sentence_len -2-total )
            output_label = [-100] + output_label + [-100] * (self.max_sentence_len -1-total )
            if not ( len(mask) == self.max_sentence_len and len(sentence_id)== self.max_sentence_len and len(output_label) == self.max_sentence_len ):
                print(sentence)
                assert False
            return torch.tensor(sentence_id), torch.tensor(mask) , torch.tensor(output_label)

    def ForShrinkVocab(self , sentence_id):
        for i in range(len(sentence_id)):
            if sentence_id[i] > self.shrink_vocab_size:
                sentence_id[i] = self.tokenizer.unk_token_id
        return

    def __len__(self):
        return len(self.corpus)
print('okk')

class Bert_TAPTrain(nn.Module):
    def __init__(self, model, hidden_layer_dim=768, shrink_vocab_size=10000):
        super(Bert_TAPTrain, self).__init__()
        self.bert = model # 直白传已在general corpus上预训练的BERT模型，

        # 以下等价于 BertPredictionHeadTransform // 不过是 线性 + 激活层 + 线性
        self.dense = nn.Linear(hidden_layer_dim, hidden_layer_dim)
        self.transform_act_fn = nn.GELU() # 默认
        self.LayerNorm = nn.LayerNorm(hidden_layer_dim) #, eps=config.layer_norm_eps
        self.predict_layer = nn.Linear(hidden_layer_dim , shrink_vocab_size )
        self.loss_func = nn.CrossEntropyLoss()

    def forward(self, x , mask , labels):
        # Step 1:
        hidden_state = self.bert(x,attention_mask=mask)[0] # [batch , seq_len, hidden]
        # Step 2:
        hidden_state = self.transform_act_fn( self.dense(hidden_state) )
        hidden_state = self.LayerNorm(hidden_state) # [B , seq ,embedding_dim]
        # Step 3:
        output = self.predict_layer(hidden_state) # out: [B , seq , vocab_size]
        # Step 4: 直接计算loss
        return self.loss_func(output.transpose(-2, -1) , labels )
        # equals to self.loss_func(output.view(-1, vocab_size) , labels.view(-1))

if __name__ == '__main__':
    # step 1: data and dl
    df =  pd.read_csv("/content/drive/MyDrive/ML_data/Lyrics_AfterWash.csv")
    Shrink_Size = 10000 # 预测词汇表，删去总和小于千分之三的非中文字符类型

    tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/BERT_preTrain")
    train_set = BERT_PT_Dataset(corpus=list(df['lyric']) , tokenizer=tokenizer,maxi_len=256 , shrink_vocab_size=Shrink_Size)
    train_dl = data.DataLoader(dataset=train_set, batch_size=16, drop_last=False, shuffle=True)

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    bert_model = BertModel.from_pretrained("/content/drive/MyDrive/TAPT_Bert/TAPT_11_28") # /content/drive/MyDrive/BERT_preTrain
    model = Bert_TAPTrain(model=bert_model , shrink_vocab_size=Shrink_Size)
    model.to(device)
    optimizer = torch.optim.Adam( model.parameters(), lr = 2e-5 )
    print('模型已加载!')

    print('start to train ...')
    print(device)
    import time
    start = time.time()
    Save_path = "/content/drive/MyDrive/TAPT_Bert/TAPT_11_28"
    for epoch in range(8):
        model.train()
        total_train_loss = 0
        # train
        for i, batch in enumerate(train_dl):
            x, mask, labels = batch
            input_x = x.to(device)
            mask = mask.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            loss = model(input_x , mask , labels)
            # backward and optimize
            total_train_loss += loss.item()
            loss.backward()
            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 防止梯度爆炸
            optimizer.step()
            if (i * 16) % 1000 < 16: # 训练了 1K 数据 , 16 为 batch size
                print('Epoch: {} , Batch: {}  Train Loss: {}'.format(epoch , i , loss.item())  )

        print('训练loss: ' , total_train_loss / len(train_dl))
        print( 'Epoch：{}'.format(epoch), " 总训练时间 (min)： " , (time.time() - start)//60 )

        bert_model.save_pretrained(Save_path) # 每轮都保存一波
        if epoch % 2 == 1: # 每两轮换一套完形填空
            train_set = BERT_PT_Dataset(corpus=list(df['lyric']) , tokenizer=tokenizer,maxi_len = 256 , shrink_vocab_size=Shrink_Size)
            train_dl = data.DataLoader(dataset=train_set, batch_size=16, drop_last=False, shuffle=True)
            print('数据已重加载，重新Random！！！')