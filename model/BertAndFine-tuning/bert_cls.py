# -*- coding: utf-8 -*-
"""BERT_cls.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Nn3BOfm8OpwhJ__CZMnNQetDgkKxqGA
"""

# 利用 BERT 搞一波二分类
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import data

import transformers
from transformers import  BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification

import pandas as pd
import numpy as np
from sklearn.metrics import confusion_matrix
import random

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

# !pip install transformers

# 数据划分
def split_train_test(data, test_ratio=0.3):
    '''
    划分 train.tsv 为训练集/测试集
    :param data:
    :param test_ratio: 划分比例
    :return:
    '''
    # data: dataframe
    np.random.seed(123)
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

"""# 模型及dataLoader及train/predict"""

# 模型
class ClassifyLayer(nn.Module):
    def __init__(self, hidden_layer_dim=768,
                 class_num = 2,
                 If_CNN = False , cnn_channel = 16 , pool_left_dim = 2):
        super(ClassifyLayer, self).__init__()
        # 上一步BERT输出:  [Batch size, seq_len , hidden_layer_dim]
        self.If_CNN = If_CNN
        if If_CNN:
            self.conv_unigram = nn.Conv1d(in_channels= hidden_layer_dim, out_channels=cnn_channel,
                                          kernel_size=1, stride=1, padding=0)
            self.conv_2gram = nn.Conv1d(in_channels= hidden_layer_dim, out_channels=cnn_channel,
                                        kernel_size=2, stride=1, padding=1)
            self.conv_3gram = nn.Conv1d(in_channels= hidden_layer_dim, out_channels=cnn_channel,
                                        kernel_size=3, stride=1, padding=1)
            self.conv_4gram = nn.Conv1d(in_channels= hidden_layer_dim, out_channels=cnn_channel,
                                        kernel_size=4, stride=1, padding=2)
            self.pool = nn.AdaptiveAvgPool1d(pool_left_dim) # UNDO
            self.batch_norm = nn.BatchNorm1d(cnn_channel)
            self.dropout = nn.Dropout(p=.5)
            self.linear =  nn.Linear( cnn_channel * pool_left_dim * 4 , class_num )
        else:
            self.linear = nn.Linear(hidden_layer_dim , class_num)

    def forward(self, x):
        if not self.If_CNN:
            return  self.linear(x) #
        else:
            x = x.transpose(-2, -1)  # [Batch , wv_dim , seq_len  ]
            # 1,2,3,4 -gram 卷积核 （假装表示 1/2/3/4 号词语）
            x1 = self.pool(F.relu(self.batch_norm(self.conv_unigram(x))))
            x2 = self.pool(F.relu(self.batch_norm(self.conv_2gram(x))))
            x3 = self.pool(F.relu(self.batch_norm(self.conv_3gram(x))))
            x4 = self.pool(F.relu(self.batch_norm(self.conv_4gram(x))))

            new_x = torch.cat([x1, x2, x3, x4], dim=-1)  # 合并
            new_x = new_x.view(new_x.size(0), -1)
            output = self.dropout(new_x)
            return self.linear(output)  # 无需多加softmax


class BERT_cls(nn.Module):
    def __init__(self, model ,hidden_layer_dim=768 , class_num = 2 , cnn_channel = 32 , pool_left_dim = 4, If_CNN = False):
        super(BERT_cls, self).__init__()
        self.bert = model # 直白传已预训练模型
        self.classify = ClassifyLayer(hidden_layer_dim , class_num , If_CNN = If_CNN ,
                        cnn_channel = cnn_channel , pool_left_dim = pool_left_dim)
        self.If_CNN = If_CNN
        self.stop_bert_train = False

    def forward(self, x , mask):
        if self.If_CNN:
            output = self.bert(x,attention_mask=mask)[0] # [batch , maxi_seq_len , hidden size]
            if self.stop_bert_train:
                output = output.detach()
        else:
            output = self.bert(x, attention_mask=mask)[1]
        return self.classify(output)

    def chg_BackPropagation_state(self , new_state: bool):
        self.stop_bert_train = new_state
        return

# *------- DataLoader -------------*
# [PAD] : 0
class BertDataSet(data.Dataset):
    def __init__(self, dat_X, tokenizer ,maxi_len = 400, target=None):
        self.dat = dat_X
        self.target = target
        self.max_sentence_len = maxi_len
        self.tokenizer = tokenizer
        return

    def __getitem__(self, index):
      '''
      :param index:
      :return:
      '''
      # sentence ： [101 开始符, xxx ,xx ,x ,x , 0 , 0 ,... , 102 终止符]
      encode_dict = self.tokenizer.encode_plus(self.dat[index] ,padding = 'max_length' ,truncation=True , add_special_tokens=True , max_length=self.max_sentence_len )
      sentence = encode_dict['input_ids']
      mask = encode_dict['attention_mask']
      # sentence = self.tokenizer.encode( self.dat[index] , add_special_tokens=True , max_length=self.max_sentence_len ,pad_to_max_length=True)
      if self.target is not None:
          return sentence ,mask , self.target[index]
      else:
          return sentence , mask

    def __len__(self):
      return len(self.dat)

def collate_fn_bert(data_batch):
    data =  [s[0] for s in data_batch]
    masks = [s[1] for s in data_batch]
    target = [ s[-1]  for s in data_batch]
    return (torch.LongTensor(data) , torch.LongTensor(masks) ,torch.LongTensor(target))

"""# 训练一波"""

def train(model, train_X, loss_func, optimizer):
    model.train()
    total_loss = 0
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    for i, batch in enumerate(train_X):
      x, mask, y = batch
      input_x  = x.to(device)
      mask = mask.to(device)
      y = y.to(device)
      optimizer.zero_grad()
      # backward and optimize
      output = model(input_x , mask)
      loss = loss_func(output, y)
      loss.backward()
      optimizer.step()
      total_loss += loss.item() * output.shape[0]
      if (i + 1) % 300 == 0:
          print("Step [{}/{}] Train Loss: {:.4f}".format(i + 1, len(train_X), loss.item()))

    print(total_loss / len(train_X.dataset))
    return total_loss / len(train_X.dataset)



def predict(model,test_X,SequenceClassifierOutput = False):
    # 这个prediction 写得还是太烂
    model.eval()
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    with torch.no_grad():
        for i , batch in enumerate(test_X):
            x, mask, y = batch
            input_x  = x.to(device)
            mask = mask.to(device)
            output = model( input_x , mask ) # 假设输出为概率值类的东西
            if SequenceClassifierOutput:
              output = output[0] # 就是一个刚过完线性层的东西
            if i == 0:
                pred_prob = output.detach().to('cpu').numpy()
                y_true = y
            else:
                pred_prob = np.concatenate((pred_prob, output.detach().to('cpu').numpy() ))
                y_true = np.concatenate((y_true,y))
    return pred_prob , y_true


def setup_seed(seed):
     torch.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     np.random.seed(seed)
     random.seed(seed)
     torch.backends.cudnn.deterministic = True

def main():
    # 设置随机数种子
    setup_seed(128)
    print(random.random())
    # STEP 1: 数据集
    df = pd.read_csv("/content/drive/MyDrive/ML_data/Lyrics_AfterWash.csv")
    # 二分类
    df = df[(df['sentiment'] == 0) | (df['sentiment'] == 2)]
    df['sentiment'] = df['sentiment'].map({0: 0, 2: 1})
    print(len(df))


    print(len(df))
    train_dat, test_dat = split_train_test(df, 0.3)
    y_true = list(train_dat['sentiment'].apply(lambda x: 0 if x == 0 else 1))
    tokenizer = BertTokenizer.from_pretrained("/content/drive/MyDrive/BERT_preTrain/")
    train_set = BertDataSet(dat_X=list(train_dat['lyric']),maxi_len = 300 ,tokenizer=tokenizer , target=y_true)
    train_dl = data.DataLoader(dataset=train_set, batch_size=16, drop_last=False, shuffle=True,collate_fn=collate_fn_bert)

    y_test = list(test_dat['sentiment'].apply(lambda x: 0 if x == 0 else 1))
    test_set = BertDataSet(dat_X=list(test_dat['lyric']), maxi_len = 300 ,tokenizer=tokenizer, target=y_test)
    test_dl = data.DataLoader(dataset=test_set, batch_size=16, drop_last=False, shuffle = False, collate_fn=collate_fn_bert)

    print(len(train_dl.dataset) , len(train_dl) )

    # STEP 3: 模型、训练所需
    bert_model = BertModel.from_pretrained("/content/drive/MyDrive/Bert_Save_Model/TAPT_Bert")
    model = BERT_cls(model=bert_model, If_CNN = True)

    # # 哈工大 or 清华
    # model = BertForSequenceClassification.from_pretrained(
    #         "hfl/chinese-bert-wwm-ext",  # "/content/drive/MyDrive/BERT_preTrain/",  # Use the 12-layer BERT model, with an uncased vocab.
    #         num_labels=2,  # The number of output labels--2 for binary classification.
    #         output_attentions=False,  # Whether the model returns attentions weights.
    #         output_hidden_states=False,  # Whether the model returns all hidden-states.
    #         )

    model.to(device)
    optimizer = torch.optim.Adam( model.parameters(), lr = 2e-5 ) #
    loss_func = nn.CrossEntropyLoss()
    print('模型加载完成!')


    # STEP 4: 训练
    assert True
    print('start to train' + '*' * 20)
    for epoch in range(7):
        import time
        start = time.time()
        if 1 <= epoch <= 2: # (1,3)
          model.chg_BackPropagation_state(new_state = True) # 不回传bert梯度
        else:
          model.chg_BackPropagation_state(new_state = False)
        train(model=model, train_X=train_dl, loss_func=loss_func, optimizer=optimizer)
        end = time.time()
        # predict test dataset
        preb_prob , y_true = predict(model , test_dl)
        # print(preb_prob[0:10])
        y_pred = np.argmax( preb_prob , axis = -1)
        C =confusion_matrix( y_true=y_true , y_pred=y_pred )
        print("epoch {},Run time:{} , 测试集准确率:{}".format(epoch, end - start, C.trace() / C.sum()))
        print('P 、R 、F1: ' , C[0,0] / (C[0,0]+C[1,0]) , C[0,0]/(C[0,0]+C[0,1]) , 2*C[0,0] / (2*C[0,0]+C[0,1]+C[1,0])  )
        print(C)

    # # STEP 4: 训练
    # 抄一波现成代码
    assert False # 另一种训练方式
    class_num = 2
    for epoch in range(6):
        print('*' * 20)
        print( '*' * 5 , 'new epoch:  ' , epoch)
        print('*' * 20)
        model.train()
        total_train_loss = 0
        device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        C = np.array([[0] * class_num for _ in range(class_num)])
        # train
        for i, batch in enumerate(train_dl):
            x , mask, y = batch
            input_x = x.to(device)
            mask = mask.to(device)
            y = y.to(device)
            optimizer.zero_grad()
            result = model(input_x,
                    token_type_ids=None,
                    attention_mask=mask,
                    labels=y,
                    return_dict=True)

            loss = result.loss
            logits = result.logits
            # backward and optimize
            total_train_loss += loss.item()
            loss.backward()
            # torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 防止梯度爆炸
            optimizer.step()

            # 评估
            y_pred = np.argmax(logits.detach().cpu().numpy(), axis=1).flatten()
            y_true = y.to('cpu').numpy()
            tmp_C = confusion_matrix(y_true=y_true,y_pred=y_pred)
            C += tmp_C


        print('train loss:' , total_train_loss / len(train_dl)) # 那一点小偏差无所谓
        print('Train: accuracy: ' , C.trace() / C.sum())
        print(C)

        # test dataSet; validation
        model.eval()
        C = np.array([[0] * class_num for _ in range(class_num)])
        with torch.no_grad():
          for i, batch in enumerate(test_dl):
            x , mask, y = batch
            input_x = x.to(device)
            mask = mask.to(device)
            # y = y.to(device)
            result = model(input_x,
                    token_type_ids=None,
                    attention_mask=mask,
                    return_dict=True,)
                    #labels=y,

            logits = result.logits
            # 评估
            y_pred = np.argmax(logits.detach().cpu().numpy(), axis=1).flatten()
            y_true = y.to('cpu').numpy()
            tmp_C = confusion_matrix(y_true=y_true,y_pred=y_pred)
            C += tmp_C

        print('Test: accuracy: ' , C.trace() / C.sum())
        print('P 、R 、F1: ' , C[0,0] / (C[0,0]+C[1,0]) , C[0,0]/(C[0,0]+C[0,1]) , 2*C[0,0] / (2*C[0,0]+C[0,1]+C[1,0])  )
        print(C)
    return