# There are our bert model structure, pre-training models and some training details.

pre-training model can be accessed in the following link:

    Thanks to ShenDezhou's work

    https://drive.google.com/drive/folders/1eveX6Tnh4wpR_bRC7FVxtNcVtqYJbG1Y?usp=sharing 
      ( From https://github.com/ShenDezhou/Chinese-PreTrained-BERT#%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD )
    
    TAPT:(task-adaptive pre-training)
    
    https://drive.google.com/drive/folders/1Zwrf-4jbGyh7JtOqWN43SqpiCLSj_yJG?usp=sharing

our training details,such as optimizer, learning rate, batch size, maximum sequence length,  are the same as the py files we uploaded.
