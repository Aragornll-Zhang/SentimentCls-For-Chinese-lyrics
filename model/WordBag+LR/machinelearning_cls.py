# -*- coding: utf-8 -*-
"""MachineLearning_Cls.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/156gzfJQ5zPfQsDvi39BZ331qGkxfHVxB
"""

import pandas as pd
import jieba
import re
import collections
import numpy as np
from sklearn.linear_model._logistic import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix


# 配置变量
TWO_SIDE = True # 二分类
threshold = 5 # 每个词在训练集的最小出现次数
MOST_N = 8000 # 每种情感类别特定词汇最大数目
N_gram = 1
If_shrink_wordbag = False # 缩减各个情感之间的"公共词"
If_Wyy = False
If_neg_prefix = True

classify_way = 1 # 1: 逻辑回归 ； 2: SVM


def split_train_test(data, test_ratio=0.3):
    '''
    划分 train.tsv 为训练集/测试集
    :param data:
    :param test_ratio: 划分比例
    :return:
    '''
    # data: dataframe
    np.random.seed(123)
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

# s-VSM 
def test_s_VSM():
    file_path = "/content/drive/MyDrive/ML_data/Lyrics_A_f.csv"
    df = pd.read_csv(file_path)
    df = df[(df['sentiment']==0)|(df['sentiment'] == 2) ]
    df['sentiment'] = df['sentiment'].map({0:0 , 2:1})
    train_dat , test_dat = split_train_test(df , 0.3)
    X = np.array( list( train_dat['feature'].apply(lambda x : eval(x) ) ) )
    y = np.array( train_dat['sentiment'])
    clf = SVC(gamma='auto',probability=True)
    clf.fit(X , y)
    C = confusion_matrix(y_true=y , y_pred=clf.predict(X))
    print(C)
    print(C.trace() / C.sum())

    X_test = np.array( list( test_dat['feature'].apply(lambda x : eval(x) ) ) )
    y_test = np.array( test_dat['sentiment'])
    C = confusion_matrix(y_true=y_test , y_pred=clf.predict(X_test))
    print(C.trace() / C.sum())

    from sklearn import metrics
    y_test = 1 - y_test
    y_pred_prob = clf.predict_proba(X_test)[:, 0]
    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob) # 把悲伤当成正例
    auc = metrics.auc(fpr, tpr)
    print(auc)
    return

# test_s_VSM() # 为了瘸子里面拔将军

def main():
    # 停用词表, 否定前缀
    stop_words = set()
    stop_words.add(' ')  # 空格
    file_path = "/content/drive/MyDrive/ML_data/停用词表.txt"
    with open(file_path, encoding='utf-8') as f:
        for elem in f.readlines():
            stop_words.add(elem.strip('\n'))
    print(len(stop_words))

    Negative_prefix = set(['不', '不是', '不再', '非', '无'])
    jieba.suggest_freq(('难再'), tune=True)


    file_path = "/content/drive/MyDrive/ML_data/Lyrics_AfterWash.csv"
    df = pd.read_csv(file_path)

    if If_Wyy:
      df['sentiment'] = df['label']
    # *------------  若仅仅 积极/消极 二分类 -------------*
    if TWO_SIDE:
      df = df[(df['sentiment']==0)|(df['sentiment'] == 2) ]
      df['sentiment'] = df['sentiment'].map({0:0 , 2:1})
    # *---------  END  ----------*
    # 划分训练/测试集
    train_dat , test_dat = split_train_test(df , 0.3)

    N = len(df['sentiment'].value_counts())
    W_bags = [ collections.Counter() for _ in range(N) ] # 词频表

    for i in range(len(train_dat)): # len(df)
        label = train_dat['sentiment'].iloc[i]
        lyric = train_dat['lyric'].iloc[i]

        word_bag = W_bags[label]
        pattern = re.compile( "(:|：|-)" )
        if N_gram == 1:
            for sentence in lyric.split('。'):
                if pattern.search( sentence ):
                    continue
                all_cut = jieba.lcut(sentence)
                neg_pref_flag = False
                for i in range(len(all_cut)):
                    elem = all_cut[i]
                    if elem in stop_words:
                        continue
                    if If_neg_prefix:
                      if elem in Negative_prefix:
                        neg_pref_flag = True
                        continue
                      elif neg_pref_flag:
                        neg_pref_flag = False
                        elem = '_' + elem # 否定前缀
                    word_bag[elem] += 1
        else: # N_grams
          for sentence in lyric.split('。'):
              if pattern.search( sentence ):
                  continue
              all_cut = jieba.lcut(sentence) # 句子切分完后
              if len(all_cut) <= N_gram:
                  word_bag[' '.join(all_cut)] += 1
              for i in range(len(all_cut) - N_gram + 1):
                  elem = ' '.join( all_cut[ i : (i+N_gram)] )
                  word_bag[elem] += 1

    All_words = {}
    # N-gram + MI
    index = 0
    for i in range(len(W_bags)):
        word_bag = W_bags[i]
        for item , count in word_bag.most_common(MOST_N):

            # print('get here!')
            if count >= threshold and item not in All_words:
              if If_shrink_wordbag: # 压缩与删减
                  cnt = 0
                  for wb in W_bags:
                    if item in wb:
                      cnt += 1
                  if cnt != len(W_bags): # 若全有，则删去该词汇
                     All_words[item] = index
                     index += 1
              else:
                All_words[item] = index
                index += 1
        print(index)
    assert index == len(All_words)


    # 获得 dat_X
    def process(lyric , N_gram = 1):
        x = [0] * len(All_words)
        if N_gram == 1:
            for sentence in lyric.split('。'):
                if pattern.search( sentence ):
                    continue

                if If_neg_prefix:
                  all_cut = jieba.lcut(sentence)
                  neg_pref_flag = False
                  for i in range(len(all_cut)):
                    elem = all_cut[i]
                    if elem in stop_words:
                        continue
                    if elem in Negative_prefix:
                      neg_pref_flag = True
                      continue
                    elif neg_pref_flag and If_neg_prefix:
                      neg_pref_flag = False
                      elem = '_' + elem # 否定前缀

                    if elem in All_words:
                      x[All_words[elem]] = 1
                else:
                  for elem in jieba.cut(sentence):
                      if elem in All_words:
                          x[All_words[elem]] = 1
        else: # N-gram
            for sentence in lyric.split('。'):
                if pattern.search( sentence ):
                    continue
                all_cut = jieba.lcut(sentence) # 句子切分完后
                if len(all_cut) <= N_gram and ' '.join(all_cut) in All_words:
                    x[All_words[ ' '.join(all_cut) ]] += 1
                    break
                for i in range(len(all_cut) - N_gram + 1):
                    elem = ' '.join( all_cut[ i : (i+N_gram)] )
                    if elem in All_words:
                      x[All_words[elem]] += 1
        return x

    X = np.array( list( train_dat['lyric'].apply(process) ) )
    y = np.array( train_dat['sentiment'])
    print('get data!')


    """# 分类！！！"""

    if classify_way == 1:
      # 逻辑回归
      clf = LogisticRegression( max_iter= 500 , C=0.2 ) # auto 已经默认 multinormal
    elif classify_way == 2:
      clf = SVC(gamma='auto') # 不太好收敛
    # clf = SVC(gamma='auto')
    clf.fit(X , y) # array or sparse
    # sum(clf.predict(X) == y) / len(y) # mean accuracy
    print( '训练集分对率:' , clf.score(X , y) ) # mean accuracy
    confuse = confusion_matrix(y_true=y , y_pred=clf.predict(X))
    print(confuse)

    X_test = np.array( list( test_dat['lyric'].apply(process) ) )
    y_test = np.array( test_dat['sentiment'])
    confuse = confusion_matrix(y_true=y_test , y_pred=clf.predict(X_test))
    print(confuse)
    print('测试集准确率:' , confuse.trace()/confuse.sum() )
    C = confuse
    print('P 、R 、F1: ' , C[0,0] / (C[0,0]+C[1,0]) , C[0,0]/(C[0,0]+C[0,1]) , 2*C[0,0] / (2*C[0,0]+C[0,1]+C[1,0])  )
    return


"""# 卷准确率 （选泛化能力最强的超参）"""

def cross_validation(X , y , model, k_fold = 5):
    N = len(X)
    predict_cnt = 0
    sample = np.random.choice(k_fold,N,replace=True)
    for i in range(k_fold):
        # index_list[i] 为test
        train_X , train_y = X[np.where(sample != i)] , y[np.where(sample != i)]
        test_X , test_y = X[np.where(sample == i)], y[np.where(sample == i)]
        model.fit(train_X , train_y) # fit 的系数重新另算
        predict_cnt += np.sum( model.predict(test_X) == test_y)
      
    return predict_cnt / N

def get_best_superPara(X , y):
    last = None
    for c in [0.2 + i*0.1 for i in range(10)]:
      c = 1.2
      clf = LogisticRegression( max_iter= 500,C = c)
      tmp = cross_validation(X,y,clf,k_fold=10)
      if last is None or last > tmp:
        record_best = c
      print('l2:',c, tmp )
      last = tmp
    print(record_best)
    return


"""# 测试对不含标签数据的效果"""
def testOnAll():
    # 总测试集
    df_test = pd.read_csv('/content/drive/MyDrive/ML_data/All_lyrics_1026.csv')
    X_test = np.array( list( df_test['lyric'].apply(process) ) )
    y_pred=clf.predict(X_test)

    y_pred_prob = clf.predict_proba(X_test)
    y_pred_prob = y_pred_prob[:, 0] # 取出"积极列"的概率值

    df_test.insert(1 , '预测情感', y_pred)
    df_test.insert(2, '预测为积极的概率', y_pred_prob)
    df_test['预测情感'] = df_test['预测情感'].map({0:'积极',1:'悲伤'}) # 四分类时 df_test['预测情感'].map({0:'积极',1: '亢奋', 2:'悲伤' ,3:'安静'})
    print(df_test.head)

    df = df_test[['id','预测情感','预测为积极的概率','歌曲名称','情感标签']]
    df.rename(columns={'id':'歌曲ID','情感标签':'原始情感标签'})
    df.to_csv('/content/drive/MyDrive/ML_data/基于歌词的情感二分类结果_10_26.csv' , index = False)
    return


"""# 聚类"""
def Kmeans():
  from sklearn.cluster import KMeans

  kmeans = KMeans( n_clusters= 2 )
  kmeans.fit(X)
  print( kmeans.score(X) )

  from sklearn import metrics
  print( metrics.silhouette_score(X , kmeans.predict(X) ) )

  kmeans = KMeans( n_clusters= 4 )
  kmeans.fit(X)
  print( kmeans.score(X) )

  from sklearn import metrics
  print( metrics.silhouette_score(X , kmeans.predict(X) ) )


if __name__ == '__main__':
    main()